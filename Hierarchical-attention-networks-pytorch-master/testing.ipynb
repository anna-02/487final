{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils import get_max_lengths, get_evaluation\n",
    "from src.dataset import MyDataset\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import numpy as np  # linear algebra\n",
    "import os  # accessing directory structure\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import train_a\n",
    "from train_a import train_model\n",
    "from train_a import get_optimizer\n",
    "from src.final_fnn_model import FFNN\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (context): HierAttNet(\n",
       "    (word_att_net): WordAttNet(\n",
       "      (lookup): Embedding(400001, 50)\n",
       "      (gru): GRU(50, 50, bidirectional=True)\n",
       "    )\n",
       "    (sent_att_net): SentAttNet(\n",
       "      (gru): GRU(100, 128, bidirectional=True)\n",
       "      (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       "      (sent_softmax): Softmax(dim=None)\n",
       "      (fc_softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (reply): HierAttNet(\n",
       "    (word_att_net): WordAttNet(\n",
       "      (lookup): Embedding(400001, 50)\n",
       "      (gru): GRU(50, 50, bidirectional=True)\n",
       "    )\n",
       "    (sent_att_net): SentAttNet(\n",
       "      (gru): GRU(100, 128, bidirectional=True)\n",
       "      (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       "      (sent_softmax): Softmax(dim=None)\n",
       "      (fc_softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=178, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = 'train.csv'\n",
    "test_set = 'test.csv'\n",
    "word2vec_path = 'glove.6B.50d.txt'\n",
    "\n",
    "batch_size = 64\n",
    "hidden_layer_size = 128\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "training_params = {\"batch_size\": batch_size,\n",
    "                   \"shuffle\": True,\n",
    "                   \"drop_last\": True}\n",
    "test_params = {\"batch_size\": batch_size,\n",
    "               \"shuffle\": False,\n",
    "               \"drop_last\": False}\n",
    "\n",
    "\n",
    "# max_word_length, max_sent_length = get_max_lengths(train_set)\n",
    "max_word_length = 36\n",
    "max_sent_length = 8\n",
    "training_set = MyDataset(\n",
    "    train_set, word2vec_path, max_sent_length, max_word_length)\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "val_generator = DataLoader(training_set, **training_params)\n",
    "testing_set = MyDataset(test_set, word2vec_path,\n",
    "                        max_sent_length, max_word_length)\n",
    "test_generator = DataLoader(test_set, **test_params)\n",
    "\n",
    "net = FFNN(hidden_layer_size, batch_size, max_word_length)\n",
    "\n",
    "optim = get_optimizer(net, lr=1e-3, weight_decay=0)\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Start Training ------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/Users/annaablove/Desktop/eecs487/487final/Hierarchical-attention-networks-pytorch-master/src/word_att_model.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = element_wise_mul(f_output, output.permute(1, 0))\n",
      "/Users/annaablove/Desktop/eecs487/487final/Hierarchical-attention-networks-pytorch-master/src/sent_att_model.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = F.softmax(output)\n",
      "/Users/annaablove/Desktop/eecs487/487final/Hierarchical-attention-networks-pytorch-master/src/final_fnn_model.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = torch.cat((cont_out, rep_out), dim=1).requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits tensor([[nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan],\n",
      "        [nan, nan]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 6/125 [00:14<04:54,  2.48s/it]\n",
      "  0%|          | 0/10 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8k/gl791hln58jfjf42l9wnlv140000gn/T/ipykernel_40391/2714711477.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_a\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/eecs487/487final/Hierarchical-attention-networks-pytorch-master/train_a.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, training_generator, testing_generator, batch_size, optim)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/eecs487/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/eecs487/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from train_a import plot_loss\n",
    "\n",
    "best_model, stats = train_a.train_model(net, training_generator, training_generator, batch_size, optim)\n",
    "plot_loss(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 3 columns\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows and 10 columns\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aaccf985712f3365a0665cd97e1d700c84dd895d6e91bdf4f720763f515c96a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('eecs487')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
